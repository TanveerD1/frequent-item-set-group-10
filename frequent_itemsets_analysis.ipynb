{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7844b2bb",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "90855b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8abf3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53c9c99a",
   "metadata": {},
   "source": [
    "# 1. Simulating Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f381621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Patricia] Pool of 30 Supermarket Items\n",
    "items = [\n",
    "    \"milk\", \"bread\", \"butter\", \"eggs\", \"cheese\", \"apples\", \"bananas\", \"chicken\", \"beef\", \"pasta\",\n",
    "    \"rice\", \"flour\", \"sugar\", \"salt\", \"pepper\", \"onions\", \"tomatoes\", \"carrots\", \"potatoes\", \"cereal\",\n",
    "    \"oil\", \"juice\", \"yogurt\", \"tea\", \"coffee\", \"chocolate\", \"cookies\", \"soap\", \"shampoo\", \"toothpaste\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7239b527",
   "metadata": {},
   "source": [
    "### Generate Transaction Data, 3k transactions, 30 with 2-7 items each\n",
    "setting seed as 69 for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b44f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Suezan]Set a random seed for reproducibility of the transaction generation\n",
    "random.seed(69)\n",
    "#[Suezan] Initialize an empty list to store the generated transactions\n",
    "transactions = []\n",
    "#[Suezan Loop 3000 times to generate 3000 transactions\n",
    "for _ in range(3000):\n",
    "#[Suezan] Randomly select the number of items for the current transaction (between 2 and 7)\n",
    "    num_items = random.randint(2, 7)\n",
    "    # Randomly select 'num_items' unique items from the 'items' pool for the current transaction\n",
    "    transaction = random.sample(items, num_items)\n",
    "    # Adding the generated transaction to the list of all transactions\n",
    "    transactions.append(transaction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc27bc",
   "metadata": {},
   "source": [
    "### Save raw Transactions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50e9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Arlen] Convert the list of transactions into a Pandas DataFrame\n",
    "transactions_df = pd.DataFrame(transactions)\n",
    "#[Arlen] Create the 'data' directory if it doesn't exist to store output files\n",
    "os.makedirs('data', exist_ok=True)\n",
    "#[Arlen] Save the raw transactions DataFrame to a CSV file without the index\n",
    "transactions_df.to_csv('data/supermarket_transactions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45568fe7",
   "metadata": {},
   "source": [
    "# 2. Preprocessing Data: Using One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc6f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Calvin] Initialize the TransactionEncoder\n",
    "encoder = TransactionEncoder()\n",
    "#[Calvin]  Fit the encoder to the transactions and transform them into a one-hot encoded array\n",
    "encoded_array = encoder.fit_transform(transactions)\n",
    "#[Calvin]  Convert the one-hot encoded array into a Pandas DataFrame, using item names as column headers\n",
    "onehot_df = pd.DataFrame(encoded_array, columns=encoder.columns_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e7c33",
   "metadata": {},
   "source": [
    "# 3. Genreating Frequent Itemsets with Apriori from the previous Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f18d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Samantha] Apply the Apriori algorithm to find frequent itemsets with a minimum support of 0.05\n",
    "#[Samantha] 'use_colnames=True' ensures that item names are used instead of column indices\n",
    "frequent_itemsets = apriori(onehot_df, min_support=0.05, use_colnames=True)\n",
    "#[Samantha] Add a 'length' column to the DataFrame, representing the number of items in each itemset\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e7de0f",
   "metadata": {},
   "source": [
    "#### Saving the topn 10 frequent itemsets to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1168cac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     support     itemsets  length\n",
      "6   0.162000     (cereal)       1\n",
      "3   0.159667      (bread)       1\n",
      "26  0.159333        (tea)       1\n",
      "27  0.158667   (tomatoes)       1\n",
      "2   0.156667       (beef)       1\n",
      "1   0.155000    (bananas)       1\n",
      "19  0.154000     (pepper)       1\n",
      "9   0.153333  (chocolate)       1\n",
      "5   0.152333    (carrots)       1\n",
      "18  0.152000      (pasta)       1\n"
     ]
    }
   ],
   "source": [
    "#[Calvin] Sort frequent itemsets by support in descending order and display the top 10\n",
    "print(frequent_itemsets.sort_values(by='support', ascending=False).head(10))\n",
    "#[Calvin] Save all frequent itemsets (not just filtered by length) to a CSV file\n",
    "#[Calvin] This ensures all discovered frequent itemsets are recorded.\n",
    "frequent_itemsets.to_csv('data/frequent_itemsets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef5a44",
   "metadata": {},
   "source": [
    "#### Closed Itemsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Tanveer] Create a copy of frequent itemsets to identify closed itemsets\n",
    "closed_itemsets = frequent_itemsets.copy()\n",
    "#[Tanveer] Initialize a boolean mask to mark closed itemsets\n",
    "closed_mask = []\n",
    "\n",
    "#[Tanveer] Iterate through each frequent itemset to check if it's closed\n",
    "for i, row in closed_itemsets.iterrows():\n",
    "    #  Assume the current itemset is closed until proven otherwise\n",
    "    is_closed = True\n",
    "    # Iterate through all other frequent itemsets to find supersets\n",
    "    for j, other_row in frequent_itemsets.iterrows():\n",
    "        # Check if 'row' is a subset of 'other_row' (i.e., 'other_row' is a superset of 'row')\n",
    "        # And if they have the exact same support value\n",
    "        if row['itemsets'] < other_row['itemsets'] and row['support'] == other_row['support']:\n",
    "            # If such a superset with the same support exists, the current itemset is NOT closed\n",
    "            is_closed = False\n",
    "            # No need to check further supersets for this 'row', break the inner loop\n",
    "            break\n",
    "    # Append the determination (True/False) for the current itemset to the mask\n",
    "    closed_mask.append(is_closed)\n",
    "\n",
    "#[Tanveer] Filter the closed_itemsets DataFrame using the generated mask\n",
    "closed_itemsets = closed_itemsets[closed_mask]\n",
    "#[Tanveer] Save the identified closed itemsets to a CSV file\n",
    "closed_itemsets.to_csv(\"data/closed_itemsets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd06ac",
   "metadata": {},
   "source": [
    "#### Maximal Itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd68d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[patricia] Create a copy of frequent itemsets to identify maximal itemsets\n",
    "maximal_itemsets = frequent_itemsets.copy()\n",
    "# [patriciaInitialize a boolean mask to mark maximal itemsets\n",
    "maximal_mask = []\n",
    "\n",
    "# [Tanveer] Iterate through each frequent itemset to check if it's maximal\n",
    "for i, row in maximal_itemsets.iterrows():\n",
    "    # Assume the current itemset is maximal until proven otherwise\n",
    "    is_maximal = True\n",
    "    # Iterate through all other frequent itemsets to find supersets\n",
    "    for j, other_row in frequent_itemsets.iterrows():\n",
    "        # Check if 'row' is a subset of 'other_row' (i.e., 'other_row' is a superset of 'row')\n",
    "        # If a superset exists, the current itemset is NOT maximal\n",
    "        if row['itemsets'] < other_row['itemsets']:\n",
    "            is_maximal = False\n",
    "            # No need to check further supersets for this 'row', break the inner loop\n",
    "            break\n",
    "    # Append the determination (True/False) for the current itemset to the mask\n",
    "    maximal_mask.append(is_maximal)\n",
    "\n",
    "#[Samantha]Filter the maximal_itemsets DataFrame using the generated mask\n",
    "maximal_itemsets = maximal_itemsets[maximal_mask]\n",
    "#[Samantha] Save the identified maximal itemsets to a CSV file\n",
    "maximal_itemsets.to_csv(\"data/maximal_itemsets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815da974",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
